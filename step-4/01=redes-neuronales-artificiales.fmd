/h2 Redes neuronales artificiales
---/n
Las redes neuronales artificiales difieren drásticamente de las biológicas. Las RRNN artificiales pretenden imitar el comportamiento de las biológicas, para ello, se reduce todo a números y operaciones matemáticas./n
Las simples operaciones dentro de una red neuronal /n

/h3 Origen/n
La inteligencia es una virtud que tenemos los seres humanos. Somos capaces de pensar, comprender y asimilar como ningún ser es capaz. Gracias a esta habilidad que tenemos, podemos construir edificios, investigar, hipotetizar, inventar, aprender./n
Aunque hay unos pocos animales que tienen parte de esa facultad también, como los gorilas, somos la única especie que conocemos con esa capacidad tan desarrollada./n
Pero, ¿y si pudiésemos crear inteligencia? ¿y si pudiésemos inventar algo que pudiese pensar por sí mismo? Aristóteles pensaba que la capacidad de razonar era exclusiva del ser humano, ¿y si no lo fuese?/n
Los científicos de la década de los 40 ya habían probado a crear programas que tuviesen fórmulas muy complejas o que se aplicasen reiteradamente, pero no habían conseguido crear un sistema que tuviese la habilidad de usar la lógica como lo hacemos nosotros./n
Pero entonces, en 1943, unos científicos tomaron un enfoque radicalmente diferente para intentar solventar este problema. Podrían, en vez de intentar dar con un algoritmo complejo que simulase la lógica, intentar recrear un modelo matemático de cómo nosotros, los seres humanos, pensamos./n
Warren McCulloch y Walter Pitts fueron los pioneros que se adentraron en el estudio de la neurociencia computacional, al publicar ese año A Logical Calculus of the Ideas Immanent in Nervous Activity.
El modelo que ellos presentaron ha sufrido muchos cambios con los años, y difiere en gran medida de cómo funcionan las redes neuronales artificiales actuales, pero sentó las bases y el interés de la población por las capacidades de esta nueva tecnología./n
La razón por la que ese trabajo de investigación entre los dos científicos tuvo tanto éxito es por que consiguieron demostrar que simulando las neuronas y las conexiones neuronales de forma binaria, era posible realizar las tres funciones lógicas fundamentales./n
Eso quería decir que el mecanismo de razonamiento humano podía ser simplificado a esa simulación de neuronas y enlaces. A este primer modelo, la primera red neuronal artificial de la Historia, se le denominó la neurona de McCulloch-Pitts./n

/h3 Neuronas/n
Ahora nos adentraremos en el funcionamiento de las RRNN artificiales./n

/img /br Representación gráfica de una red neuronal artificial. /br Figura 1.

La figura 1 muestra la representación de una RRNN artificial. Lo primero que llama la atención son unos grandes círculos: las neuronas./n
Las neuronas son entidades que contienen un número, llamado sesgo. Se encargan de recibir información por los enlaces detrás (a la izquierda) de ellas, modificar esa información y transmitirla para delante./n
En la mayoría de las redes neuronales artificiales, a diferencia de las biológicas, las neuronas que están adelante no pueden comunicar información a las de atrás, pero sí vice versa. Por esa razón, una vez que la información llega al final de la red neuronal, su actividad cesa por completo y no hay nada que hacer./n
Aunque sí que hay redes neuronales más avanzadas, como las que usa ChatGPT o3-mini, que puede devolver información a neuronas previas, esta tecnología presenta muchos problemas (relacionados con la coordinación y la velocidad) por lo que no hablaremos de ella./n
Las neuronas, como puedes observar en la ilustración, están organizadas en capas verticales que pueden tener un número arbitrario de neuronas. Existen tres tipos de capas:/n
    o Capa de entrada: las primeras neuronas, que no tienen enlaces previos, se activan cuando se les da información de entrada como si viniese de enlaces. 
    o Capas ocultas: las neuronas en estas capas transformarán la información original en otra, que nos es más valiosa. Puede haber un número indefinido de capas ocultas.
    o Capa de salida: las últimas neuronas, una vez que han transformado la información, dan como resultado, como salida, esa información./n
Las redes neuronales necesitan un proceso de aprendizaje para aprender que información de toda la información de entrada es la valiosa, para devolverla en las neuronas de salida./n
Al final, una red neuronal artificial de este tipo, denominado perceptrón multicapa, no es más que una gran función matemática. Tú das unos valores de entrada (como los argumentos en una función), por dentro se realizan cálculos, y se devuelven unos datos de salida./n
Es por esa razón, por que es una gran función matemática, que los ordenadores pueden simular este tipo de redes neuronales rápidamente./n

/h3 Enlaces/n
Las conexiones, o enlaces, son las líneas que conectan las neuronas. Estos enlaces transportan la información de una neurona transmisora a una receptora (dirección indicada por la flecha). Para los perceptrones multicapa hay algunas restricciones sobre a dónde pueden ir estos enlaces:/n
    o Los enlaces no pueden volver a neuronas previas.
    o Los enlaces no pueden comunicar dos neuronas en la misma capa.
    o Los enlaces no pueden transmitir información a través de otras capas de neuronas./n
Estos enlaces, aparte de transmitir información, también la modifican. Las conexiones almacenan un número, denominado peso, que modificará el valor que recibe de la neurona transmisora y se lo dará a la receptora./n

/h3 Cálculo simple/n
Observa la siguiente red neuronal.

/img /br RRNN artificial simple con valores. /br Figura 2.

En la figura 2 he añadido los valores de sesgo de las neuronas dentro de las neuronas y los valores de peso de los enlaces encima de ellos./n
Antes de calcular nada, tenemos que conocer las operaciones que usaremos para calcular el resultado de la red neuronal:/n
    o Los sesgos: los sesgos en las neuronas sumaran el valor entrante con su valor de sesgo y se lo transmiten al todos los enlaces salientes.
    o Los pesos: los pesos en los enlaces multiplican el número entrante por su valor de peso y se lo transmiten a la neurona receptora./n
Anotaremos todos los valores, para no tener que estar recurriendo a la imagen y para poder usar nomenclatura específica:/n
/math
b^{(0)}_0=2 \qquad b^{(1)}_0=7 \qquad b^{(2)}_0=1 \\
w^{(1)}_{0,0}=3 \qquad w^{(2)}_{0,0}=9
/math/n
Los sesgos se nombran con la nomenclatura /_b^{(\text{nº de capa})}_{\text{nº de neurona}}/_. Tanto el número de capa como el de neurona empieza en /_0/_, siendo cero la primera capa o la primera neurona de la capa./n
Los pesos se nombran de la siguiente forma: /_w^{(\text{nº de capa receptora})}_{\text{nº de receptora, " " transmisora}}/_. En el caso de los sesgos y los pesos, el superíndice se escribe entre parenténsis para diferenciarlo de los exponentes./n
Vamos a operar esta red neuronal con un número cualquiera, como /_x/_. Para arrancar la red neuronal, daremos este valor de entrada a la única neurona de la capa de entrada./n
Lo primero que sucede es que al llegar a la neurona, /_x/_ se encuentra con el valor /_b^{(0)}_0/_ y por tanto se suma:/n
/math
\begin{align*}
r(x) &= x + b^{(0)}_0
\end{align*}
/math/n
He llamado también a la transformación de la red neuronal como la función /_r(x)/_. Acto seguido, el resultado /_x + b^{(0)}_0/_ se propaga por el único enlace, y se multiplica por su peso:/n
/math
\begin{align*}
r(x) &= (x + b^{(0)}_0) \cdot w^{(1)}_{0,0}
\end{align*}
/math/n
y al llegar a la siguiente neurona se le suma su sesgo:/n
/math
\begin{align*}
r(x) &= (x + b^{(0)}_0) \cdot w^{(1)}_{0,0} + b^{(1)}_0
\end{align*}
/math/n
siguiente conexión:/n
/math
\begin{align*}
r(x) &= ((x + b^{(0)}_0) \cdot w^{(1)}_{0,0} + b^{(1)}_0) \cdot w^{(2)}_{0,0}
\end{align*}
/math/n
hasta la última neurona, la neurona de salida:/n
/math
\begin{align*}
r(x) &= ((x + b^{(0)}_0) \cdot w^{(1)}_{0,0} + b^{(1)}_0) \cdot w^{(2)}_{0,0} + b^{(2)}_0
\end{align*}
/math/n
Podríamos simplificar, pero no es necesario. Esta función matemática modela los resultados de nuestra red neuronal. Podemos darle algunos valores:/n
/math
r(-5)=-17 \qquad r(-1)=91 \qquad r(0)=118 \qquad r(1)=145 \qquad r(5)=253 \\
r(-100)=-2582 \qquad r(-10)=-152 \qquad r(10)=388 \qquad r(100)=2818
/math/n

/> Si representamos gráficamente esta función, veremos que es una función lineal con pendiente /_27/_, y es que todas las RRNN artificiales con una sola neurona por capa se simplifican a una función de grado 1. En cambio, una RRNN con dos neuronas por capa puede llegar a simular cualquier función continua.

En realidad, las redes neuronales artificiales no funcionan de esta forma por dentro. Generalmente cuentan con unas funciones de activación que limitan el valor que las neuronas pueden transmitir./n
La más usada se denomina ReLU (por sus siglas en inglés, Rectified Linear Unit). Esta función se define como:/n
/math
ReLU(x)=\max (0, x)
/math/n
y pretende simular la activación y desactivación de las neuronas biológicas, que ante un impulso pueden no responder. Nuestra ecuación de la RRNN quedaría así con la función de activación:/n
/math
\begin{align*}
r(x) &= ReLU(ReLU(x + b^{(0)}_0) \cdot w^{(1)}_{0,0} + b^{(1)}_0) \cdot w^{(2)}_{0,0} + b^{(2)}_0
\end{align*}
/math
A la última neurona no se le aplica la función de activación para mantener el conjunto de resultados posibles. Además, obtenemos resultados diferentes:/n
/math
r(-5)=64 \qquad r(-1)=91 \qquad r(0)=118 \qquad r(1)=145 \qquad r(5)=253 \\
r(-100)=64 \qquad r(-10)=64 \qquad r(10)=388 \qquad r(100)=2818
/math/n

/h3 Cálculo/n
Si tenemos más de una neurona por capa, recurriremos a conceptos matemáticos más avanzados para operar nuestra red neuronal./n

/img /br Perceptrón multicapa. /br Figura 3.

Vamos a nombrar los sesgos y pesos que hay en esta red neuronal. Como puedes comprobar viendo la imagen, hemos aumentado un poco el número de neuronas pero el número de variables ha aumentado drásticamente. Empezaremos nombrando los sesgos:/n
/math
\underbrace{b^{(0)}_0 \qquad b^{(0)}_1}_\text{Sesgos capa 0} \qquad \underbrace{b^{(1)}_0 \qquad b^{(1)}_1}_\text{Sesgos capa 1} \qquad \underbrace{b^{(2)}_0}_\text{Sesgo capa 2}
/math/n
y ahora los pesos:/n
/math
\underbrace{w^{(1)}_{0,0} \qquad w^{(1)}_{1,0}}_\text{Enlaces neurona 0 capa 0} \qquad \underbrace{w^{(1)}_{0,1} \qquad w^{(1)}_{1,1}}_\text{Enlaces neurona 1 capa 0} \qquad \\
\underbrace{w^{(2)}_{0,0} \qquad w^{(2)}_{0,1}}_\text{Enlaces neuronas capa 1}
/math/n
Ya no podemos computar el resultado de forma lineal como hicimos antes. No con variables. Pero sí si usamos álgebra lineal. Para ello, reuniremos todos los valores de sesgo de cada capa en un vector:
/math
b^{(0)} = \begin{pmatrix}
b^{(0)}_0 \\
b^{(0)}_1
\end{pmatrix}
\qquad
b^{(1)} = \begin{pmatrix}
b^{(1)}_0 \\
b^{(1)}_1
\end{pmatrix}
\qquad
b^{(2)} = \begin{pmatrix}
b^{(2)}_0
\end{pmatrix}
/math/n
Aunque estemos organizando todos estos valores en vectores, realmente no existe un significado geométrico para lo que estamos haciendo. Los vectores aquí son únicamente listas de números. Algo similar haremos con los pesos, pero los organizaremos en matrices:/n
/math
w^{(1)} = \begin{pmatrix}
w^{(1)}_{0,0} & w^{(1)}_{0,1} \\
w^{(1)}_{1,0} & w^{(1)}_{1,1}
\end{pmatrix}
\qquad
w^{(2)} = \begin{pmatrix}
w^{(2)}_{0,0} & w^{(2)}_{0,1}
\end{pmatrix}
/math/n
Una matriz es una lista de vectores. Como ya hemos considerado que un vector es una lista, una matriz sigue la idea de ser una lista de listas./n
Dentro de esta lista de listas, la primera lista es aquella que involucra todas las conexiones de la primera neurona, la segunda involucra todas de la segunda neurona.../n
Además, podemos multiplicar vectores por matrices con facilidad, en la siguiente sección . Los ordenadores son capaces de hacer estas operaciones muy rápido./n
Vamos a construir ahora la función que nos dará el resultado de esta red neuronal, lo haremos sin profundizar en los valores que hay dentro de las matrices y los vectores. Primero denotaremos a la variable de entrada como /_X/_, en mayúscula para simbolizar que es un vector:/n
/math
\begin{align*}
f(X) &= X
\end{align*}
/math/n
Lo primero que se encuentra el vector /_X/_, como vemos en la imagen, es el vector de sesgos de la capa /_0/_, por lo que se lo añadimos y aplicamos la función de activación:/n
/math
\begin{align*}
f(X) &= ReLU(X + b^{(0)})
\end{align*}
/math/n
La función de activación /_ReLU()/_ de un vector aplica a cada componente, es decir:/n
/math
X = \begin{pmatrix}
X_1 \\
X_2 \\
\vdots \\
X_n
\end{pmatrix}
\longrightarrow 
ReLU(X)= \begin{pmatrix}
ReLU(X_1) \\
ReLU(X_2) \\
\vdots \\
ReLU(X_n)
\end{pmatrix}
/math/n
Lo siguiente que se encuentra nuestra red neuronal son los pesos de la capa /_0/_ a la capa /_1/_, es decir, /_w^{(1)}/_. Multiplicamos:/n
/math
\begin{align*}
f(X) &= ReLU(X + b^{(0)}) \cdot w^{(1)}
\end{align*}
/math/n
Únicamente nos queda por aplicar, en este orden; /_b^{(1)}/_, /_ReLU()/_, /_w^{(2)}/_ y /_b^{(2)}/_:/n
/math
\begin{align*}
f(X) &= ReLU(ReLU(X + b^{(0)}) \cdot w^{(1)} + b^{(1)}) \cdot w^{(2)} + b^{(2)}
\end{align*}
/math/n

/h3 Multiplicaciones vector-matriz
